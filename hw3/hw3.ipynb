{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4830bb",
   "metadata": {},
   "source": [
    "#### Advanced Statistics for Data Science (Spring 2022)\n",
    "# Home Assignment 3\n",
    "#### Topics:\n",
    "- Statistical Estimation\n",
    "- Hypothesis Testing in one and two samples\n",
    "\n",
    "#### Due: 25/04/2022 by 18:30\n",
    "\n",
    "#### Instructions:\n",
    "- Write your name, Student ID, and date in the cell below. \n",
    "- Submit a copy of this notebook with code filled in the relevant places as the solution of coding excercises.\n",
    "- For theoretic excercises, you can either write your solution in the notebook using $\\LaTeX$ or submit additional notes.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85aaf7f",
   "metadata": {},
   "source": [
    "**Name**: Ofir Nesher\n",
    "\n",
    "**Student ID**: 204502926\n",
    "\n",
    "**Date**: 16/04/2022\n",
    "\n",
    "$\n",
    "\\newcommand{\\Id}{{\\mathbf{I}}}  \n",
    "\\newcommand{\\SSE}{\\mathsf{SSE}}\n",
    "\\newcommand{\\SSR}{\\mathsf{SSR}}\n",
    "\\newcommand{\\MSE}{\\mathsf{MSE}}\n",
    "\\newcommand{\\simiid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ex}{\\mathbb E}\n",
    "\\newcommand{\\var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}[2]{{\\mathrm{Cov}  \\left(#1, #2 \\right)}}\n",
    "\\newcommand{\\one}[1]{\\mathbf 1 {\\left\\{#1\\right\\}}}\n",
    "\\newcommand{\\SE}[1]{\\mathrm{SE} \\left[#1\\right]}\n",
    "\\newcommand{\\reals}{\\mathbb R}\n",
    "\\newcommand{\\Ncal}{\\mathcal N}\n",
    "\\newcommand{\\abs}[1]{\\ensuremath{\\left\\vert#1\\right\\vert}}\n",
    "\\newcommand{\\rank}{\\operatorname{rank}}\n",
    "\\newcommand{\\tr}{\\operatorname{Tr}}\n",
    "\\newcommand{\\diag}{\\operatorname{diag}}\n",
    "\\newcommand{\\sign}{\\operatorname{sign}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3caf0",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080e735",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1 (Variance Estimation)\n",
    "\n",
    "Consider the variance estimate\n",
    "$$\n",
    "s^2 = \\frac{1}{n-1} \\sum_{i=1}^n(y_i - \\bar{y})^2. \n",
    "$$\n",
    "If $Y_i \\simiid \\Ncal(\\mu,\\sigma^2)$, then \n",
    "$$\n",
    "\\frac{n-1}{\\sigma^2}s^2 \\sim \\chi^2_{n-1}.\n",
    "$$\n",
    "1. Use this information to derive a $1-\\alpha$ coinfidence interval for $s^2$ (express $L$ and $U$ in terms of $s^2$, $n$, and the relevant quantiles of the $\\chi^2$ distribution). \n",
    "2. For $n = 2,\\ldots,10$ and $\\alpha=0.05$, report on the lower ($L$) and upper ($U$) values of the coinfidence interval in terms of $s^2$. \n",
    "3. How large $n$ must be to obtain a $0.95$ coinfidence interval of size $0.1s^2$? \n",
    "\n",
    "The point: the number of degrees of freedom needed for a reasonable ($10\\%$ range) estimate of the variance can be very large. Sometimes, much larger than our data permits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facaef7e-faa6-445f-8404-518c0c51e98a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38545db4-cd50-4551-a918-ad8690e36b5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.\n",
    "Denote $T = \\frac{n-1}{\\sigma^2}s^2$ --> $T \\sim \\chi^2_{n-1}$\n",
    "\n",
    "Derive a $1-\\alpha$ coinfidence interval for $s^2$:\n",
    "\n",
    "$$\n",
    "(1-\\alpha) = Pr(|T| < {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}) =\n",
    "$$\n",
    "$$\n",
    "Pr(-{\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}< T < {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}) = \n",
    "$$\n",
    "$$\n",
    "Pr(-{\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}< \\frac{n-1}{\\sigma^2}s^2 < {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}) =\n",
    "$$\n",
    "$$\n",
    "Pr(-\\frac{\\sigma^2 {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}}{n-1} < s^2 < \\frac{\\sigma^2 {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}}{n-1})\n",
    "$$\n",
    "\n",
    "Where $U=\\frac{\\sigma^2 {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}}{n-1}$ and $L=-\\frac{\\sigma^2 {\\chi^2_{n-1}}^{(1-\\frac{\\alpha}{2})}}{n-1} \\rightarrow -U$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f9c92-38aa-44f5-8d81-c69877844ff1",
   "metadata": {},
   "source": [
    "#### 2.\n",
    "We need to plug each n into the distribution's PDF and find $L$ & $U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95c5d0-f798-411e-927e-44ff328642c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for n in range(2,11):\n",
    "    degrees_of_freedom = n - 1\n",
    "    print(stats.chi2(degrees_of_freedom).pdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1106728-c264-4519-9b65-bf402e97b3b0",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847f750",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Problem 2 (Correlated Data)\n",
    "\n",
    "Suppose that $Y_1,\\ldots,Y_n$ has each mean $\\mu$ and variance $\\sigma^2$, but \n",
    "$$\n",
    "\\rho_{ij} := \\mathrm{Corr}(Y_i,Y_j) = \\begin{cases}\n",
    "1 & i=j \\\\\n",
    "\\rho & |i-j| = 1 \\\\\n",
    "0 & |i-j| > 1\n",
    "\\end{cases}.\n",
    "$$\n",
    "This situation arise when an observation $i$ may depend to some extent on only the previous observation’s white noise: an one-lag \"holdeover effect\". This is also known as the \"lag-1 moving average\" model (MA(1)). \n",
    "\n",
    "1. Show that:\n",
    "  - $$\\mathrm{Var}(\\bar{Y}) = \\frac{\\sigma^2}{n}(1+ 2\\rho \\frac{n-1}{n})$$\n",
    "Namely, positive correlation increases varaince. Hint: use that $\\mathrm{Var}(U+V) = \\mathrm{Var}(U) + \\mathrm{Var}(V) + 2 \\mathrm{Cov}(U,V)$ and induction or recursive computation over $n$. Another option is to write $Y = \\Sigma^{1/2}Z$ where $Z\\sim \\Ncal(0,I)$ and $\\Sigma^{1/2}$ is symmetric with  $\\Sigma^{1/2}\\Sigma^{1/2} = \\Sigma$ has the desired covariance stracture.  \n",
    "\n",
    "  - $$\\qquad \\ex[{s^2}] = \\sigma^2(1 - 2\\rho/n)$$\n",
    "  where $s^2$ is the standard varince estiamte. \n",
    "Namely, with positive correlation the \"variety\" in the data is smaller. \n",
    "\n",
    "  - **(Bonus)** The t-statistic statisfies\n",
    "$$\n",
    "t = \\sqrt{n} \\frac{\\bar{Y}-\\mu}{s} \\to \\Ncal(0,1 + 2 \\rho),\\quad n \\to \\infty\n",
    "$$\n",
    "Hint: you may use the following version of Slutsky's Theorem: for two sequences of RV U_n and V_n, if $U_n \\overset{D}{\\to} U$ and $V_n \\overset{p}{\\to} c$ (constant), then $ V_n U_n \\overset{D}{\\to} cU$\n",
    "2. Verify your answer to the first two items in 1 using simulations. Use `nMonte = 10000` problem instances. In each instance, use a sample size of `n = 10` with $\\sigma=1$ and $\\rho \\in \\{\\pm 0.1, \\pm 0.3, \\pm0.5\\}$. The function `genrate_correlated_data` below generates noramlly distributed data satisfying the correlation model above. \n",
    "\n",
    "3. Suppose $\\rho>0$\n",
    " - Derive a $1-\\alpha$ confidence interval based on $s$ and the $t$-distribution with $n-1$ DoF. Does your interval \n",
    " contains the value of $\\mu$ more or less often than $1-\\alpha$? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Suppose that we reject $H_0\\,:\\,\\mu = \\mu_0$ whenever $t$ exceeds the critical value $t_{n-1}^{1-\\alpha/2}$. Would our P-value be too small or too large? Would we reject more or less often then $\\alpha$ if the null $\\mu = \\mu_0$ is true? Verify using a simulation with `nMonte = 10000` problem instances of sampes size `n=100`. Also use $\\alpha=0.05$, $\\rho=.25$, $\\sigma =1$, and $\\mu_0=2$. \n",
    " \n",
    " - Would your answer to the preivous two items change if $\\rho < 0$? how?\n",
    "\n",
    "The point: correlation in our data is bad because it makes us make wrong descisions. The effect of correlation is much worst than non-nomrality since the latter diminishes with $n$ due to the CLT. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29896f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def genrate_correlated_data(n: int, rho: float, mu: float, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate samples from the model:\n",
    "    Yi ~ N(mu, sigma^2) and Corr(Yi,Yj) = ( i == j ) + rho * ( abs( i - j ) == 1 )\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    :n:     sample size\n",
    "    :rho:   desired one lag correlation between samples\n",
    "    :mu:    mean\n",
    "    :sigma: standard deviation\n",
    "    \n",
    "    \"\"\"\n",
    "    assert sigma > 0\n",
    "    \n",
    "    # build desired covariance matrix\n",
    "    Sig = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i==j : \n",
    "                Sig[i,j] = 1\n",
    "            if np.abs(i-j) == 1:\n",
    "                Sig[i,j] = rho\n",
    "                Sig[j,i] = rho\n",
    "                \n",
    "    # get matrix square root of covariance matrix:\n",
    "    Sig_sqrt = np.linalg.cholesky(sigma**2 * Sig)\n",
    "    \n",
    "    # sample from the standard normal dist. and transform \n",
    "    # so that the result is a normal vector with the desired \n",
    "    # covaraince structure\n",
    "    return mu + Sig_sqrt @ np.random.randn(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3a1a7-bb50-4f2c-896c-7ae548e8cc38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baafa2-c3c3-45b6-88a6-fee544a39dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297be1fa-bfdb-425f-84e5-c3dff5651bba",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59474d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Problem 3 (Regression and Hypothesis Testing)\n",
    "\n",
    "The dataset for this problem is available in the file temp_TLV_beach.csv, which was taken directly from the meterological service website (https://ims.data.gov.il/ims/1). \n",
    "\n",
    "We consider monitoring changes in rainfall/precipitation over the years at Station 136320 located at Tel-Aviv beach area. \n",
    "To do so, we will set up a standard linear model with $p = 3$ features, where for dates (times) $t \\in \\{0,1,\\ldots,366\\}$ (we have 366 for leap years) we set\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 \\cos( 2\\pi(t/365)) + \\beta_2 \\sin( 2\\pi(t/365)) + \\epsilon_t,\\qquad t=1,\\ldots,n. \n",
    "\\label{eq:model} \\tag{2}\n",
    "$$\n",
    "(note that the dataset does not contain measurments from all days in the range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8964705b",
   "metadata": {},
   "source": [
    "1) Set $y_t = \\texttt{Rainfall}$. \n",
    " - Plot $y_t$ versus $t=$`Date` and identify winter times.\n",
    "\n",
    "- Find the LS regression coefficients $\\beta$; plot the fitted response $\\hat{y}_t$ over time along with the original response $y_t$. \n",
    "\n",
    " - Test whether the fitted model significantly improves on the trival model $y_t = \\beta'_0 + \\epsilon_t$.\n",
    " \n",
    " - For each parameter $p$, report the P-value for testing $H_0\\,:\\,\\hat{\\beta}_p = 0$ and indicate whether this parameter is \n",
    "significantly different than $0$ at level $\\alpha = 0.01$. \n",
    "(for this item, you can either evaluate everything from the formulas provded in class or use a statistical package like `statsmodels`)\n",
    "\n",
    "You may use the code below to format the `Date` column correctly and extract other relevant information from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2851b65",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/01_raw/rainfall_TLV_beach.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13628/3134305659.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath_to_data_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/01_raw/rainfall_TLV_beach.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_data_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%d-%m-%Y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/01_raw/rainfall_TLV_beach.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_data_file = \"../data/01_raw/rainfall_TLV_beach.csv\"\n",
    "data = pd.read_csv(path_to_data_file)\n",
    "\n",
    "data['Date'] = pd.to_datetime(data.Date, format=\"%d-%m-%Y\")\n",
    "data['DayOfYear'] = data.Date.dt.day_of_year\n",
    "data['Month'] = data.Date.dt.month\n",
    "data['Year'] = data.Date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c121b7",
   "metadata": {},
   "source": [
    "2) We would like to test whether future data follows a similar distribution to past data. Consider two datasets modeled by\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = Z \\beta + \\epsilon,\\qquad y_{new} = Z_{new} \\beta + \\epsilon_{new}\n",
    "\\label{eq:model} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $Z \\in \\reals^{m\\times p}$ and $Z_{new} \\in \\reals^{n\\times p}$ are the given design matrices which both assume to have rank $p$. We also assume that $\\epsilon$ and $\\epsilon_{new}$ are independent. We will think of $(Z, y)$ as the initial data pair and $(Z_{new},y_{new})$ as the new data.\n",
    "\n",
    "Let $\\hat{\\beta} = (Z \\top Z)^{-1}Z^\\top y$ be the usual least-squares (LS) estimate on the initial data. Define the predicted values as\n",
    " $$\n",
    " \\hat{y}_{new} := Z_{new} \\hat{\\beta}\n",
    " $$\n",
    " (note that $\\hat{y}_{new}$ is not the LS estiamte of $y_{new}$ from $Z_{new}$)\n",
    " \n",
    " - Show that $\\mathrm{Cov}(y-\\hat{y},y_{new} - \\hat{y}_{new})=0$\n",
    " \n",
    " - Assume $\\epsilon_{new} \\sim \\Ncal(0,\\sigma^2 I_n)$. Find a (symmetric, positive definite) matrix $M \\in \\reals^{n \\times n}$ so that\n",
    " $$\n",
    " M(y_{new} - \\hat{y}_{new}) \\sim \\Ncal(0, \\sigma^2 I_n).\n",
    " $$\n",
    " \n",
    "- Give the distribution of the ratio\n",
    "$$\n",
    "\\begin{equation}\n",
    "A:= \\frac{\\frac{1}{n}\\left\\| M(Y_{new} - \\hat{Y}_{new})\\right\\|^2}{\\frac{1}{m-d} \\left\\| Y  - \\hat{Y} \\right\\|^2 }\n",
    "\\label{eq:A} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "under the null hypothesis:\n",
    "$$\n",
    "H_0\\,:\\,\\begin{cases} Y = Z \\beta + \\epsilon,\\qquad Y_{new} = Z_{new} \\beta + \\epsilon_{new} \\\\\n",
    "\\epsilon \\sim \\Ncal(0, \\sigma^2 I_m),\\qquad \\epsilon_{new} \\sim \\Ncal(0, \\sigma^2 I_n) \\\\\n",
    "\\text{$\\epsilon$ and $\\epsilon_{new}$ are independent}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We now consider implementing a series of hypothesis tests about whether daily rainfall is remaining consistent over the years or whether it is changing in some meaningful way. \n",
    "\n",
    "- For each of the years 2010, 2011,...,2021, repeat the following. Define a data matrix $Z$ using the features in $\\eqref{eq:model}$ consisting of all dates prior to that year (so that for 2010, $Z$ will be\n",
    "a data matrix for the years 2005–2009, for 2011, $Z$ will be the data for years 2005-2011, and\n",
    "so on). Define the responses $y$ to consist of rainfall for the given years. Define the new data matrix $Z_{new} \\in \\reals^{n \\times p}$ to consist of the $n$ days of measurements in the given year ($n\\leq 366$) and the responses $y_{new}$ to be the rainfall in those days. For this data, compute the statistic $A$ in $\\eqref{eq:A}$ and its p-value, that is, conditional on\n",
    "$A = a$, report\n",
    "$$\n",
    "p := \\Pr[A \\geq a] \\quad \\text{under $H_0$}\n",
    "$$\n",
    "Plot the P-values for each of the years and also print their values. Discuss briefly. \n",
    "- Suppose that you obtained a very small p-value of some year, say $p \\approx 10^{-5}$. Does rejecting the null hypothesis necessarily mean that the distribution of rainfall is changing over time? explain in 2-3 sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9199b",
   "metadata": {},
   "source": [
    "3) Consider the total amount of rainfall within each month. Suppose that we assume that there is no change in the distribution over time across years, but we suspect that December is usuallly rainier than February. Design a test procedure that checks whether this is true. Use two apporaches:\n",
    " - Two-sample t-test \n",
    " - Paired t-test \n",
    " - Which approach seems more approproate here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c747e8-bc0f-4e58-92ce-5e6fa89cf0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Answers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9deff3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
