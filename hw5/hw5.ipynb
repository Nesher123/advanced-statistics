{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c987d913",
   "metadata": {},
   "source": [
    "#### Advanced Statistics for Data Science (Spring 2022)\n",
    "# Home Assignment 5\n",
    "#### Topics:\n",
    "- Contrasts\n",
    "- Multiple TestinFalse-Discovery Rate\n",
    "- Simple Regression\n",
    "\n",
    "#### Due: 24/05/2022 by 18:30\n",
    "\n",
    "#### Instructions:\n",
    "- Write your name, Student ID, and date in the cell below. \n",
    "- Submit a copy of this notebook with code filled in the relevant places as the solution of coding excercises.\n",
    "- For theoretic excercises, you can either write your solution in the notebook using $\\LaTeX$ or submit additional notes.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26e332",
   "metadata": {},
   "source": [
    "\n",
    "**Name**: \n",
    "\n",
    "**Student ID**:\n",
    "\n",
    "**Date**:\n",
    "\n",
    "$\n",
    "\\newcommand{\\Id}{{\\mathbf{I}}}  \n",
    "\\newcommand{\\SSE}{\\mathsf{SSE}}\n",
    "\\newcommand{\\SSR}{\\mathsf{SSR}}\n",
    "\\newcommand{\\MSE}{\\mathsf{MSE}}\n",
    "\\newcommand{\\simiid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ex}{\\mathbb E}\n",
    "\\newcommand{\\var}{\\mathrm{Var}}\n",
    "\\newcommand{\\Cov}[2]{{\\mathrm{Cov}  \\left(#1, #2 \\right)}}\n",
    "\\newcommand{\\one}[1]{\\mathbf 1 {\\left\\{#1\\right\\}}}\n",
    "\\newcommand{\\SE}[1]{\\mathrm{SE} \\left[#1\\right]}\n",
    "\\newcommand{\\reals}{\\mathbb R}\n",
    "\\newcommand{\\Ncal}{\\mathcal N}\n",
    "\\newcommand{\\abs}[1]{\\ensuremath{\\left\\vert#1\\right\\vert}}\n",
    "\\newcommand{\\rank}{\\operatorname{rank}}\n",
    "\\newcommand{\\tr}{\\operatorname{Tr}}\n",
    "\\newcommand{\\diag}{\\operatorname{diag}}\n",
    "\\newcommand{\\sign}{\\operatorname{sign}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc7ca3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e247683",
   "metadata": {},
   "source": [
    "## Problem 1 (Multiple testing in Practice)\n",
    "#### Continuation of Problem 4 from HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3f0df",
   "metadata": {},
   "source": [
    "In HW4, Problem 4 you used ANOVA to measure the effect of the winary (``winary``) on the quality (``points``) of wines from veriaty (``veriaty``) ``Cabernet Sauvignon``. Using the same dataset (reduced to Israeli wines and a specific veriaty). You used all t-tests involving pairs of winaries and reported pairs found significant after a Bonfferoni's correction. In this question, you will use Binjamini-Hochberg's (BH) FDR controlling procedure as an alternative to Bonfferoni. \n",
    " 1. Using HW4, Problem 4, consider P-values from all t-tests of pairs of winaries. Sort the P-values from small to large and plot the smallest 10% of the sorted list of P-values on a stem or a scatter plot in which the x-axis indicate the P-value rank (left most point is rank = 1, i.e., the smallest) and the y-axis the P-value's value. \n",
    " 2. Assume that different tests are independet; apply BH to select a set of winary pairs in which one winary is significantly better than the others while aiming for 0.05 false pairs. How many pairs were reported? is it more or less pairs than you discovered using Bonfferoni correction in HW4, Problem 4?\n",
    " 3. Repeat the last item without the independence assumption, i.e., use the harmonic sum correction.\n",
    " \n",
    "The point: FDR controlling using BH is a more liberal (less conservative) approach to identify significnat discoveries in the sense that it allows us to report on more discoveries if we are willing to replace family-wise error rate control with an expected false-discovery proportion guarantee.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091a28c",
   "metadata": {},
   "source": [
    "## Problem 2 (contrasts)\n",
    "#### Continuation of Problem 1\n",
    "\n",
    "Using a single t-test, test the assumption that the average of the Golan area wineries: ``Bazelet HaGolan``, ``Gamla``, ``Golan Heights Winary`` have the same quality as the average of Judean Hills area wineries: ``Katlav``, ``Psagot``, ``Shiloh Winery``, ``Titora``, ``Yatir`` (the goal here is to construct a \"contrast\", hence you should take an arithmetic average of group averages. You should only consider the size of each individual group when evaluating the varaince of the contrast).\n",
    "\n",
    "Note: you can ignore the ``province`` filed in the data becasue it contains inaccurate information. You should also ignore the fact that in reality, ``Gamla`` is produced by ``Golan Heights Winary``. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef723e7e",
   "metadata": {},
   "source": [
    "## Problem 3 (Exact size of Bonferroni's test)\n",
    "Suppose that we run multiple tests with independent data and obtain P-values $p_1,\\ldots,p_n$. We wish to test the null hypothesis:\n",
    "$$\n",
    "H_0\\,:\\,\\text{All tests are null}\n",
    "$$\n",
    "at the level $\\alpha$ (e.g., $\\alpha=0.05$). In class, we introduced Bonferroni's procedure which is equivalent to: Reject $H_0$ if $\\min p_i \\leq \\alpha/n$. \n",
    "1. Show that the  size of the test in Bonferroni's procedure is at most $\\alpha$, regardless if the hypotheses are independent or not. \n",
    "2. Assuming that the hypotheses are independent, find the exact size of the test in Bonferroni's procedure. \n",
    "3. For $\\alpha=0.05$, evalaute the difference between $\\alpha$ and the exact test's size for $n=2,...,50$. Discuss what you see. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5b806",
   "metadata": {},
   "source": [
    "## Problem 4 (Prediction in Simple Regression)\n",
    "Consider the linear model:\n",
    "$$\n",
    "    y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\qquad \\epsilon_i \\simiid \\Ncal(0,\\sigma^2)\n",
    "$$\n",
    "$$\n",
    "    Z = \\begin{pmatrix}\n",
    "    1 & x_1 \\\\\n",
    "    \\vdots & \\vdots \\\\\n",
    "    1 & x_n\n",
    "    \\end{pmatrix},\\quad \\beta=\\begin{pmatrix}\n",
    "    \\beta_0 \\\\\n",
    "    \\beta_1\n",
    "    \\end{pmatrix},\\qquad \\hat{\\beta}=(Z^\\top Z)^{-1} Z^\\top y\n",
    "$$\n",
    "Suppose we get a new data point $x_{n+1}$ and want to predict $y_{n+1}$. We want an interval in which this prediction will likely to land. In class, we used that \n",
    "$$\n",
    "\\var[\\hat{\\beta}_0 + \\hat{\\beta}_1 x] = \\frac{\\sigma^2}{n} \\left( \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{XX }}\\right)\n",
    "$$\n",
    "to obtain a confidence interval for $\\beta_0 + \\beta_1 x$, and a confidence band for all $x \\in \\reals$. In this question, you will use a similar reasoning to get a confidence interval (and bands) for $y_{n+1}$.\n",
    "1. Find the varaince of $y_{n+1} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{n+1})$ in terms of $\\sigma^2$ and $x_1,\\ldots,x_n$ and $x_{n+1}$ (you can use $\\bar{x}$ and $S_{XX}$ or any other well-defined function of $x_1,\\ldots,x_n$). Explain intuitively why it makes sense that this variance is larger than the variance of  $\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{n+1}$. \n",
    "2. Find a $1-\\alpha$ confidence interval for $y_{n+1}$. Is this interval wider or narrower than that of $\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{n+1}$? For what value of $x_{n+1}$ this interval is the narrowest?\n",
    "3. Suppose that we take the average of two responses $y$'s at the same $x_{n+1}$, say \n",
    "$$\n",
    "y_{n+1} =  \\frac{y_{n+1}^{(1)} + y_{n+1}^{(2)}}{2}, \n",
    "$$\n",
    "where\n",
    "$$\n",
    "y_{n+1}^{(1)} = \\beta_0 + \\beta_1 x_{n+1} + \\epsilon_{n+1}^{(1)}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "y_{n+1}^{(2)} = \\beta_0 + \\beta_1 x_{n+1} + \\epsilon_{n+1}^{(2)},\n",
    "$$\n",
    "where $\\epsilon_{n+1}^{(1)}$ and $\\epsilon_{n+1}^{(2)}$ are independent. Find a confidence interval for $y_{n+1}$. Is it wider or narrower than the interval in (2) ?\n",
    "\n",
    "Note: The confidence interval you derived in 2 is somewhat risky to use becasue it makes the strong assumption that $\\epsilon_{n+1}$ is normal. This is compared to, say, confidence intrvals for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ which rely on averages over all observations $y_1,\\ldots,y_n$ so we can use the Central Limit Theorem to argue for normality. Thigs gets better both in terms of varaince and normality assumption when you can take multiple measurements at the same $x_{n+1}$ and average those. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3888d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
